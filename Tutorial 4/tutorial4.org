#+TITLE: MECH 306 Tutorial 1:
#+SUBTITLE: Analysis and Graphical Representation of Data

#+LATEX_HEADER: \definecolor{bg}{rgb}{0.95,0.95,0.95}
#+LATEX_HEADER: \setminted{frame=single,bgcolor=bg,samepage=true}
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \AtBeginEnvironment{minted}{\dontdofcolorbox}
#+LATEX_HEADER: \def\dontdofcolorbox{\renewcommand\fcolorbox[4][]{##4}}
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \setlength{\parindent}{0pt}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{svg}
#+LATEX_HEADER: \usepackage{bm}

#+BEGIN_SRC ipython :session :exports none :results none
%matplotlib inline
%config InlineBackend.figure_format = 'svg'
import numpy as np
from scipy import stats
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import pandas as pd
import statsmodels.api as sm
import itertools
#+END_SRC
* Natural Frequency of a Ski
** Derivation of Polynomial Regression
Let's say you have some variable $y$ that is dependent on some variable $x$, and you expect that the relationship between $x$ and $y$ such that:
$$y = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \ldots + \beta_nx^n$$
for some integer $n$.

If you have $m$ measurements of $x$ and $y$ you can express the above formula in matrix form:
\begin{align*}
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_m \end{bmatrix} &\approx
\begin{bmatrix}
\beta_0 + \beta_1x_1 + \beta_2x_1^2 + \beta_3x_1^3 + \ldots + \beta_nx_1^n \\
\beta_0 + \beta_1x_2 + \beta_2x_2^2 + \beta_3x_2^3 + \ldots + \beta_nx_2^n \\
\beta_0 + \beta_1x_3 + \beta_2x_3^2 + \beta_3x_3^3 + \ldots + \beta_nx_3^n \\
\vdots \\
\beta_0 + \beta_1x_m + \beta_2x_m^2 + \beta_3x_m^3 + \ldots + \beta_nx_m^n
\end{bmatrix} \\
&\approx
\begin{bmatrix}
1 & x_1 & x_1^2 & x_1^3 & \ldots & x_1^n \\
1 & x_2 & x_2^2 & x_2^3 & \ldots & x_2^n \\
1 & x_3 & x_3^2 & x_3^3 & \ldots & x_3^n \\
\multicolumn{6}{c}{$\vdots$} \\
1 & x_m & x_m^2 & x_m^3 & \ldots & x_m^n
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \\ \vdots \\ \beta_n
\end{bmatrix}
\end{align*}
For brevity let's call this:
$$\mathbf{y} \approx \mathbf{G} \bm{\beta}$$

Because this equation is overconstrained, we need to convert it to a system with one unique solution, we premultiply by the transpose of $\mathbf{G}$
$$\mathbf{G}^T \mathbf{y} = \mathbf{G}^T \mathbf{G} \bm{\beta}$$
Now we can solve for $\bm{\beta}$.
$$\bm{\beta} = (\mathbf{G}^T \mathbf{G})^{-1} \cdot \mathbf{G}^T \mathbf{y}$$
** Importing the data
First let's import the data from the tutorial instructions
#+BEGIN_SRC ipython :session :exports code :results none
f = np.array([33.2, 33.4, 33.6, 33.8])
amp = np.array([1.3, 1.5, 1.4, 1.2])
#+END_SRC

Let's also calculate the normalized variables
#+BEGIN_SRC ipython :session :exports code :results none
df = 0.2
f_mean = f.mean()
x = (f - f_mean)/(0.5*df)
#+END_SRC
** Least Squares Estimation Using Matrix Operations
Let's create $\mathbf{G}$ and $\mathbf{G}^T$:
#+BEGIN_SRC ipython :session :exports code :results none
G = np.array([[1, X, X**2] for X in x])
GT = G.transpose()
#+END_SRC

Now we can calculate $\bm{\beta}$:
#+BEGIN_SRC ipython :session :exports both :results raw drawer
beta = np.linalg.inv(GT @ G) @ GT @ amp
beta
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[49]:
: array([ 1.475, -0.02 , -0.025])
:END:
** Least Squares Estimation Using Polyfit
#+BEGIN_SRC ipython :session :exports both :results raw drawer
coef = np.polyfit(x, amp, 2)
# Flip results to match our equation
np.flip(coef)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[73]:
: array([ 1.475, -0.02 , -0.025])
:END:
** Finding the natural frequency
#+BEGIN_SRC ipython :session :exports both :results raw drawer
parabola = np.poly1d(coef)
xp = np.linspace(-4, 4, 100)
fitamp = parabola(xp)

x_max = xp[np.argmax(fitamp)]
f_max = x_max*(0.5*df)+f_mean
f'{round(f_max,2)} Hz'
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[98]:
: '33.46 Hz'
:END:

** Plot of parabola against data points
#+BEGIN_SRC ipython :session :exports results :results raw drawer

plt.plot(xp, fitamp, c='r')

plt.scatter(x, amp)
plt.xlabel('Normalized frequency')
plt.ylabel('Amplitude/mm')
plt.show()

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[89]:
[[file:./obipy-resources/DgDCZd.svg]]
:END:
* Finding Non-destructive Predictors of Wood Strength
** Importing the data
#+BEGIN_SRC ipython :session :exports code :results raw drawer
data = pd.read_csv('DK.csv')
MOR = data['MOR']

# Not sure why the data file uses different names from the tutorial
MOEj = data['MOEb']
CLTa = data['Eave']
CLTm = data['Emin']
dens = data['Dens']

# Put everything of interest in single list for further processing
cors = [
        ('MOEj', MOEj, r'$\times 10^3$ psi'),
        ('CLTa', CLTa, r'$\times 10^3$ psi'),
        ('CLTm', CLTm, r'$\times 10^3$ psi'),
        ('Density', dens, r'$\times 10^3$ kg/m$^3$')]
cors = [{'name':a[0], 'data':a[1], 'units':a[2]} for a in cors]

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[307]:
:END:

** Linear Regressions Against MOR
#+BEGIN_SRC ipython :session :exports both :results output 

for i in cors:
    slope, intercept, r, p, std_err = stats.linregress(i['data'], MOR)
    n = len(i['data'])
    i['slope'] = slope
    i['intercept'] = intercept
    i['r2'] = 1 - (1 - r**2)*((n - 1)/(n - (1 + 1)))
    i['p'] = p
    i['std_err'] = std_err
    print(f'{i["name"]}:')
    print(f'R^2(adj)={i["r2"]:.3f}, p={i["p"]:.4e}, std_err={i["std_err"]:.3f}')

#+END_SRC

#+RESULTS:
: MOEj:
: R^2(adj)=0.671, p=3.5055e-33, std_err=0.403
: CLTa:
: R^2(adj)=0.503, p=1.6044e-21, std_err=0.696
: CLTm:
: R^2(adj)=0.492, p=6.3489e-21, std_err=0.801
: Density:
: R^2(adj)=0.487, p=1.2155e-20, std_err=3.611

From the data it is obvious that MOEj is the best single predictor of MOR, while density is the poorest predictor.
CLTa and CLTm are both comparable, which makes sense since the measurements themselves should be correlated.
#+BEGIN_SRC ipython :session :exports none :results none
def plot_single(name):
    cor = next(x for x in cors if x['name']==name)
    name = cor['name']
    units = cor['units']
    r2 = cor['r2']
    data = cor['data']
    intercept = cor['intercept']
    std_err = cor['std_err']
    slope = cor['slope']

    x_range = np.linspace(0.9*min(data), 1.1*max(data), 100)

    plt.suptitle(f'{name} vs MOR')
    r_bar_string = r'$\bar{R}$'

    plt.title(f'{r_bar_string}$^2 = {r2:.3f}$, $\sigma = {std_err:.3f}$', fontsize=10)
    plt.scatter(data, MOR)
    plt.plot(x_range, intercept + slope*x_range, 'r', label='best fit')
    plt.plot(x_range, intercept + (slope + std_err)*x_range, '--', color='orange', label='$+\sigma$ slope')
    plt.plot(x_range, intercept + (slope - std_err)*x_range, '--', color='lightgreen', label='$-\sigma$ slope')
    plt.xlabel(f'{name} [{units}]')
    plt.ylabel(r'MOR [$\times 10^6$psi]')
    plt.legend()
    plt.show()
#+END_SRC
*** MOEj vs MOR
#+BEGIN_SRC ipython :session :exports results :results raw drawer
plot_single('MOEj')
#+END_SRC
#+RESULTS:
:RESULTS:
# Out[422]:
[[file:./obipy-resources/ADNF8p.svg]]
:END:
*** CLTa vs MOR
#+BEGIN_SRC ipython :session :exports results :results raw drawer
plot_single('CLTa')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[384]:
[[file:./obipy-resources/ufyN2C.svg]]
:END:
*** CLTm vs MOR
#+BEGIN_SRC ipython :session :exports results :results raw drawer
plot_single('CLTm')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[385]:
[[file:./obipy-resources/LweBM5.svg]]
:END:
*** Density vs MOR
#+BEGIN_SRC ipython :session :exports results :results raw drawer
plot_single('Density')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[386]:
[[file:./obipy-resources/kD68An.svg]]
:END:
** Multiple Regressions Against MOR
#+BEGIN_SRC ipython :session :exports both :results output
for r in range(2,5):
    combinations = itertools.combinations(cors, r)
    for combination in combinations:
        names = tuple(map(lambda a: a['name'], combination))
        data = tuple(map(lambda a: a['data'], combination))
        
        X = np.array(list(data)).transpose()
        X = sm.add_constant(X)

        model=sm.OLS(MOR, X).fit()
        r2 = model.rsquared_adj
        p = model.f_pvalue
        print(f'MOEr vs. {", ".join(names)}')
        print(f'R^2(adj)={r2}, P={p}')
#+END_SRC

#+RESULTS:
#+begin_example
MOEr vs. MOEj, CLTa
R^2(adj)=0.6876556025476117, P=1.6812086423410764e-33
MOEr vs. MOEj, CLTm
R^2(adj)=0.6687703903026456, P=7.199946880326127e-32
MOEr vs. MOEj, Density
R^2(adj)=0.6688052694941247, P=7.151584607136872e-32
MOEr vs. CLTa, CLTm
R^2(adj)=0.5176533493308978, P=2.0139867592037845e-21
MOEr vs. CLTa, Density
R^2(adj)=0.5279688293442275, P=5.048785504620796e-22
MOEr vs. CLTm, Density
R^2(adj)=0.5506514911600968, P=2.1597079088216837e-23
MOEr vs. MOEj, CLTa, CLTm
R^2(adj)=0.6968290678131568, P=2.1126571554183118e-33
MOEr vs. MOEj, CLTa, Density
R^2(adj)=0.6873141960516819, P=1.4935299367519037e-32
MOEr vs. MOEj, CLTm, Density
R^2(adj)=0.6662135246984889, P=9.304956999655308e-31
MOEr vs. CLTa, CLTm, Density
R^2(adj)=0.5475618817421818, P=2.072507328259981e-22
MOEr vs. MOEj, CLTa, CLTm, Density
R^2(adj)=0.6978296091261502, P=1.1462047399621344e-32
#+end_example

From the output it's obvious that correlations containing MOEj are the best.
However, MOEj measurements depend on laboratory equipment that would not be available at lumber processing plants, and are therefore an impractical way of assesing MOR in production.

Looking only at regressions without MOEj:
#+BEGIN_SRC ipython :session :exports both :results output
models = []
for r in range(2,5):
    combinations = itertools.combinations(cors, r)
    for combination in combinations:
        names = tuple(map(lambda a: a['name'], combination))
        if 'MOEj' in names:
            continue
        data = tuple(map(lambda a: a['data'], combination))
        
        X = np.array(list(data)).transpose()
        X = sm.add_constant(X)

        model=sm.OLS(MOR, X).fit()
        r2 = model.rsquared_adj
        p = model.f_pvalue

        models.append({
            'model': model,
            'data': data,
            'names': names,
            'X': X
            })

        print(f'MOEr vs. {", ".join(names)}')
        print(f'R^2(adj)={r2:.3f}, P={p:.3e}')

#+END_SRC

#+RESULTS:
: MOEr vs. CLTa, CLTm
: R^2(adj)=0.518, P=2.014e-21
: MOEr vs. CLTa, Density
: R^2(adj)=0.528, P=5.049e-22
: MOEr vs. CLTm, Density
: R^2(adj)=0.551, P=2.160e-23
: MOEr vs. CLTa, CLTm, Density
: R^2(adj)=0.548, P=2.073e-22

From the output we can see that regressing with CLTm and density provides both the highest adjusted R value as well as the lowest P value, indicating that that model has the highest predicting power of the ones available for use.

#+BEGIN_SRC ipython :session :exports none :results none

model_dict = next(x for x in models if x['names']==('CLTm', 'Density'))
model = model_dict['model']
cltm = model_dict['data'][0]
density = model_dict['data'][1]
X = model_dict['X']

xx1, xx2 = np.meshgrid(np.linspace(cltm.min(), cltm.max(), 100),
                       np.linspace(density.min(), density.max()))
Z = model.params[0] + model.params[1]*xx1 + model.params[2]*xx2

fig = plt.figure(figsize=(12, 8))
ax = Axes3D(fig, azim=-115, elev=15)
surf = ax.plot_surface(xx1, xx2, Z,
                       cmap=plt.cm.RdBu_r,
                       alpha=0.6,
                       linewidth=0,
                       label='best fit')
# Hack to make the legend work
surf._facecolors2d = surf._facecolors3d
surf._edgecolors2d = surf._edgecolors3d

resid = MOR - model.predict(X)
ax.scatter(cltm[resid>=0], density[resid>=0], MOR[resid>=0],
           color='black', alpha=1.0, facecolor='white', label='points above best fit')
ax.scatter(cltm[resid<0], density[resid<0], MOR[resid<0],
           color='black', alpha=1.0, label='points below best fit')

x_label = r'CLTm [$\times 10^3$ psi]'
y_label = r'Density [$\times 10^3$ kg/m^3]'
z_label = r'MOR [$\times 10^6$ psi]'
ax.set_xlabel(x_label)
ax.set_ylabel(y_label)
ax.set_zlabel(z_label)
plt.legend(loc='lower center')
plt.savefig("3d.svg")
#+END_SRC

#+COMMENT: Ghetto hack to make the plot appear bigger
#+BEGIN_SRC latex
\begin{figure}
  \centering
  \makebox[\textwidth][c]{\includesvg[width=1.4\textwidth]{./3d}}
\end{figure}
#+END_SRC
